# ğŸ—ºï¸ Codebase Architecture Map

> ğŸ¤– **AI Context File**: Read this to understand the entire codebase structure and relationships

---

## ğŸ¯ System Architecture Overview

```
ğŸŒ Real-Time Scanner Pipeline
    â”‚
    â””â”€â”€ ğŸ” Step 0: Trending Events Detection
        â”‚
        â””â”€â”€ ğŸ” Step 1: Event Analysis
            â”‚
            â””â”€â”€ ğŸ” Step 2: Content Retrieval
                â”‚
                â””â”€â”€ ğŸ” Step 3: Data Normalization
                    â”‚
                    â””â”€â”€ ğŸ” Step 4: Final Integration
                        â”‚
                        ğŸ OUTPUT
```

---

## ğŸ“ Module Detailed Breakdown

### ğŸŒŸ `step_0_trending_events.py`
**Purpose**: Initial event detection and trending analysis  
**Input**: External APIs/feeds  
**Output**: Raw trending events data  
**Key Functions**: 
- Event detection algorithms
- Trend scoring mechanisms
- Data source integration

**ğŸ“„ Current Implementation Status**: 
- â“ Needs review - check for hardcoded vs dynamic prompts
- â“ API integration status unknown
- â“ Error handling needs assessment

---

### ğŸ§  `step_1_event_analysis.py`
**Purpose**: Deep analysis of detected events  
**Input**: Raw events from step_0  
**Output**: Analyzed event data with insights  
**Key Functions**:
- Event categorization
- Sentiment analysis
- Impact assessment

**ğŸ“„ Current Implementation Status**:
- â“ AI prompts usage needs review
- â“ Analysis algorithms need documentation
- â“ Performance optimization needed

---

### ğŸ“¥ `step_2_content_retrieval.py`
**Purpose**: Fetch additional content related to events  
**Input**: Analyzed events from step_1  
**Output**: Enriched events with additional content  
**Key Functions**:
- Content scraping/API calls
- Data validation
- Content filtering

**ğŸ“„ Current Implementation Status**:
- â“ Rate limiting implementation unknown
- â“ Content source diversity needs review
- â“ Caching mechanisms missing

---

### ğŸ”„ `step_3_data_normalization.py`
**Purpose**: Clean and standardize all collected data  
**Input**: Raw content from step_2  
**Output**: Normalized, structured data  
**Key Functions**:
- Data cleaning
- Format standardization
- Quality assurance

**ğŸ“„ Current Implementation Status**:
- â“ Data validation rules need review
- â“ Output format standardization needed
- â“ Error handling for malformed data

---

### ğŸ”— `step_4_final_integration.py`
**Purpose**: Final data integration and output generation  
**Input**: Normalized data from step_3  
**Output**: Final processed results  
**Key Functions**:
- Data aggregation
- Report generation
- Output formatting

**ğŸ“„ Current Implementation Status**:
- â“ Output format needs standardization
- â“ Integration with external systems unknown
- â“ Performance metrics needed

---

### ğŸ“Š `models.py`
**Purpose**: Data models and structures  
**Contains**: 
- Event data models
- Configuration classes
- Data validation schemas

**ğŸ“„ Current Implementation Status**:
- â“ Model completeness needs review
- â“ Type hints need verification
- â“ Validation logic needs testing

---

### ğŸ› ï¸ `utils.py`
**Purpose**: Shared utility functions  
**Contains**:
- Common helper functions
- Configuration management
- Logging utilities

**ğŸ“„ Current Implementation Status**:
- â“ Function documentation needed
- â“ Error handling review required
- â“ Performance optimization opportunities

---

## ğŸ’­ Prompts System

### ğŸ“ `prompts/` Directory
**Purpose**: AI prompt templates and management  
**Key Issue**: User mentioned prompts should be DYNAMIC, not hardcoded  

**ğŸš¨ Critical TODO**: 
- Review current prompt implementation
- Design dynamic prompt system
- Implement context-aware prompt generation

---

## ğŸ“Š Results & Output

### ğŸ“ `results/` Directory
**Purpose**: Store processed outputs  
**Structure**: TBD - needs standardization  

---

## ğŸ”§ Configuration & Environment

### ğŸ” `.env.template`
**Purpose**: Environment configuration template  
**Contains**: API keys, configuration parameters  

### ğŸ“‹ `requirements.txt`
**Purpose**: Python dependencies  
**Status**: â“ Needs dependency audit and cleanup  

---

## ğŸ”„ Data Flow Diagram

```
ğŸŒ External Sources
    â”‚
    â†“ ğŸ“¥ Fetch
    â”‚
ğŸŒŸ Step 0: Trending Events
    â”‚
    â†“ ğŸ“Š Raw Events
    â”‚
ğŸ§  Step 1: Analysis
    â”‚
    â†“ ğŸ“Š Analyzed Events
    â”‚
ğŸ“¥ Step 2: Content Retrieval
    â”‚
    â†“ ğŸ“Š Enriched Events
    â”‚
ğŸ”„ Step 3: Normalization
    â”‚
    â†“ ğŸ“Š Clean Data
    â”‚
ğŸ”— Step 4: Integration
    â”‚
    â†“ ğŸ“Š Final Output
    â”‚
ğŸ“Š Results Storage
```

---

## ğŸ” Dependencies & Relationships

### ğŸ”— Module Dependencies
- All steps depend on `models.py` for data structures
- All steps use `utils.py` for common functions
- Sequential dependency: step_n depends on step_(n-1)
- `prompts/` system used by analysis steps

### ğŸ¯ External Dependencies
- API services (need documentation)
- Database connections (status unknown)
- File system for results storage

---

## ğŸ“ Notes for Future AI Sessions

### ğŸ” Key Questions to Investigate:
1. **Dynamic Prompts**: How are prompts currently managed? Are they hardcoded?
2. **Error Handling**: What happens when steps fail?
3. **Configuration**: How is the system configured for different environments?
4. **Testing**: What test coverage exists?
5. **Performance**: Are there bottlenecks in the pipeline?

### ğŸš€ Recommended First Actions for New AI Sessions:
1. Read `AI_SESSION_LOG.md` for context
2. Review recent Git commits
3. Examine current prompts implementation
4. Test run the pipeline to identify issues
5. Update documentation after any changes

---

*ğŸ¤– Last Updated: 2025-06-15T18:09:46Z by Claude (Anthropic)*

